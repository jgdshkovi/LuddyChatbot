{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d40e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_folder(folder_path, file_ext=\".txt\"):\n",
    "    documents = []\n",
    "    filenames = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(file_ext):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "\n",
    "                # Force content to string in case something weird happens\n",
    "                if isinstance(content, list):\n",
    "                    content = \" \".join(str(x) for x in content)\n",
    "                elif not isinstance(content, str):\n",
    "                    content = str(content)\n",
    "\n",
    "                documents.append(content)\n",
    "                filenames.append(filename)\n",
    "\n",
    "    return documents, filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64027de",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents_from_folder('Final_scraped_txts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ba93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all documents are strings (flattening any lists accidentally included)\n",
    "documents = [\" \".join(d) if isinstance(d, list) else str(d) for d in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6788b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: cs | datascience | informatics | ois | fortune | luddy | iu | html | indiana | txt\n",
      "Topic 2: school | luddy | students | degree | data | information | university | program | iu | science\n",
      "Topic 3: blackculture | 1604350011 | fin | thats | digitalcollections | lacasa | ssw | gtidea | trustedci | jewishculture\n",
      "Topic 4: school | luddy | student | information | university | program | data | students | iu | science\n",
      "Topic 5: blackculture | 1604350011 | fin | thats | digitalcollections | lacasa | ssw | gtidea | trustedci | jewishculture\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Preprocessing\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=500, min_df=2)\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Fit LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Show top words for each topic\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_words = [words[i] for i in topic.argsort()[-10:]]\n",
    "    print(f\"Topic {idx+1}: {' | '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20a985ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgdsh\\Desktop\\SP25\\LLM Course\\Final\\final\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Assuming `documents` is a list of 600 strings\u001b[39;00m\n\u001b[32m      4\u001b[39m topic_model = BERTopic()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m topics, probs = \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m topic_model.get_topic_info()\n\u001b[32m      8\u001b[39m topic_model.visualize_topics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgdsh\\Desktop\\SP25\\LLM Course\\Final\\final\\Lib\\site-packages\\bertopic\\_bertopic.py:472\u001b[39m, in \u001b[36mBERTopic.fit_transform\u001b[39m\u001b[34m(self, documents, embeddings, images, y)\u001b[39m\n\u001b[32m    469\u001b[39m     y, embeddings = \u001b[38;5;28mself\u001b[39m._guided_topic_modeling(embeddings)\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Reduce dimensionality and fit UMAP model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m umap_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce_dimensionality\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# Zero-shot Topic Modeling\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_zeroshot():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgdsh\\Desktop\\SP25\\LLM Course\\Final\\final\\Lib\\site-packages\\bertopic\\_bertopic.py:3776\u001b[39m, in \u001b[36mBERTopic._reduce_dimensionality\u001b[39m\u001b[34m(self, embeddings, y, partial_fit)\u001b[39m\n\u001b[32m   3773\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3774\u001b[39m     \u001b[38;5;66;03m# cuml umap needs y to be an numpy array\u001b[39;00m\n\u001b[32m   3775\u001b[39m     y = np.array(y) \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3776\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mumap_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3777\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3778\u001b[39m     \u001b[38;5;28mself\u001b[39m.umap_model.fit(embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgdsh\\Desktop\\SP25\\LLM Course\\Final\\final\\Lib\\site-packages\\umap\\umap_.py:2817\u001b[39m, in \u001b[36mUMAP.fit\u001b[39m\u001b[34m(self, X, y, force_all_finite, **kwargs)\u001b[39m\n\u001b[32m   2813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform_mode == \u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2814\u001b[39m     epochs = (\n\u001b[32m   2815\u001b[39m         \u001b[38;5;28mself\u001b[39m.n_epochs_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_epochs\n\u001b[32m   2816\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2817\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_, aux_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_embed_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2820\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2821\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# JH why raw data?\u001b[39;49;00m\n\u001b[32m   2822\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2825\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2826\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33membedding_list\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m aux_data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgdsh\\Desktop\\SP25\\LLM Course\\Final\\final\\Lib\\site-packages\\umap\\umap_.py:2865\u001b[39m, in \u001b[36mUMAP._fit_embed_data\u001b[39m\u001b[34m(self, X, n_epochs, init, random_state, **kwargs)\u001b[39m\n\u001b[32m   2860\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit_embed_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, n_epochs, init, random_state, **kwargs):\n\u001b[32m   2861\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[32m   2862\u001b[39m \u001b[33;03m    replaced by subclasses. Arbitrary keyword arguments can be passed\u001b[39;00m\n\u001b[32m   2863\u001b[39m \u001b[33;03m    through .fit() and .fit_transform().\u001b[39;00m\n\u001b[32m   2864\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimplicial_set_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2867\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2868\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2869\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2870\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2871\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2872\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrepulsion_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2873\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2875\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2877\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2878\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2879\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2880\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_densmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2881\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_dens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2883\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2884\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meuclidean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ml2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2885\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2886\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2887\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgdsh\\Desktop\\SP25\\LLM Course\\Final\\final\\Lib\\site-packages\\umap\\umap_.py:1089\u001b[39m, in \u001b[36msimplicial_set_embedding\u001b[39m\u001b[34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[39m\n\u001b[32m   1086\u001b[39m n_epochs_max = \u001b[38;5;28mmax\u001b[39m(n_epochs) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_epochs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m n_epochs\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_epochs_max > \u001b[32m10\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     graph.data[graph.data < (\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / \u001b[38;5;28mfloat\u001b[39m(n_epochs_max))] = \u001b[32m0.0\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1091\u001b[39m     graph.data[graph.data < (graph.data.max() / \u001b[38;5;28mfloat\u001b[39m(default_epochs))] = \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jgdsh\\Desktop\\SP25\\LLM Course\\Final\\final\\Lib\\site-packages\\numpy\\_core\\_methods.py:44\u001b[39m, in \u001b[36m_amax\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_amax\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     43\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_maximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Assuming `documents` is a list of 600 strings\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "\n",
    "topic_model.get_topic_info()\n",
    "topic_model.visualize_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea279e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "informatics fortune ois luddy bloomington data events computer graduate news\n",
      "----------------------------------------\n",
      "Topic #2:\n",
      "txt indiana html iu luddy fortune ois informatics cs datascience\n",
      "----------------------------------------\n",
      "Topic #3:\n",
      "informatics fortune ois luddy bloomington data events computer graduate news\n",
      "----------------------------------------\n",
      "Topic #4:\n",
      "science iu students data program university information luddy degree student\n",
      "----------------------------------------\n",
      "Topic #5:\n",
      "informatics fortune ois luddy bloomington data events computer graduate news\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=500, min_df=2)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Fit LDA\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Show top words per topic\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{idx + 1}:\")\n",
    "    print(\" \".join([terms[i] for i in topic.argsort()[-10:][::-1]]))\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d92d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
